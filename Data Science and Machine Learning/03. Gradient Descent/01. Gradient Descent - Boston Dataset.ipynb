{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ef7b93d3",
   "metadata": {},
   "source": [
    "Question 1)\n",
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "\n",
    "1. Code Gradient Descent for N features and come with predictions.\n",
    "2. Try and test with various combinations of learning rates and number of iterations.\n",
    "3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "\n",
    "Read Instructions carefully -\n",
    "\n",
    "1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "4. Your score is based on coefficient of determination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd2acf",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca2e460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For creating nd arrays\n",
    "from sklearn.model_selection import train_test_split # For random spliting of dataset\n",
    "from sklearn import linear_model # For using inbuilt LinearRegression in the Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf83ba4",
   "metadata": {},
   "source": [
    "### Fetching data from Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c69add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40784991, -0.48772236, -1.2660231 , ...,  0.41057102,\n",
       "        -1.09799011, 37.9       ],\n",
       "       [-0.40737368, -0.48772236,  0.24705682, ...,  0.29116915,\n",
       "        -0.52047412, 21.4       ],\n",
       "       [ 0.1251786 , -0.48772236,  1.01599907, ..., -3.79579542,\n",
       "         0.89107588, 12.7       ],\n",
       "       ...,\n",
       "       [-0.40831101, -0.48772236,  0.24705682, ...,  0.33206621,\n",
       "        -0.33404299, 20.8       ],\n",
       "       [-0.41061997, -0.48772236, -1.15221381, ...,  0.203235  ,\n",
       "        -0.74475218, 22.6       ],\n",
       "       [ 0.34290895, -0.48772236,  1.01599907, ...,  0.38787479,\n",
       "        -1.35871335, 50.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Set for training the algorithm\n",
    "train_set = np.loadtxt('boston_train.csv', delimiter = ',')\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123ac5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.91816626, -0.48772236,  1.01599907, ...,  0.80657583,\n",
       "        -1.59755122,  1.04106182],\n",
       "       [-0.40339151, -0.48772236,  0.40609801, ..., -1.13534664,\n",
       "         0.44105193, -0.89473812],\n",
       "       [-0.4131781 , -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.50084979],\n",
       "       ...,\n",
       "       [-0.41001449,  2.08745172, -1.37837329, ..., -0.0719129 ,\n",
       "         0.39094481, -0.68167397],\n",
       "       [-0.40317611, -0.48772236, -0.37597609, ...,  1.13022958,\n",
       "         0.34007019,  0.20142086],\n",
       "       [-0.13356344, -0.48772236,  1.2319449 , ..., -1.73641788,\n",
       "        -2.93893082,  0.48877712]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set for testing the algorithm\n",
    "test_set = np.loadtxt('boston_test.csv', delimiter = ',')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set[:,:-1] # Fetching all the parameters from training_set\n",
    "y = train_set[:,-1]  # Fetching all the output from training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca0b64",
   "metadata": {},
   "source": [
    "### Creating Class for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d14acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class of Linear Regression for n features using Gradient Descent\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        print('Algorithm Initiated')\n",
    "    \n",
    "    # Defining the cost function\n",
    "    def __cost(self,x,y):\n",
    "        cost = np.zeros(self.__M)\n",
    "        for rowNo in range(self.__M):\n",
    "            cost[rowNo] = (y[rowNo]-((self.__constants*x[rowNo]).sum()))**2\n",
    "        return cost.mean()\n",
    "    \n",
    "    # Defining function for gradient descent for minimizing the cost\n",
    "    def __step_gradient_descent(self,x,y,alpha):\n",
    "        costderivative = np.zeros(self.__N)\n",
    "        for colNo in range(self.__N):\n",
    "            for rowNo in range(self.__M):\n",
    "                derivative = (-2/self.__M)*(y[rowNo]-((self.__constants*x[rowNo]).sum()))*x[rowNo,colNo]\n",
    "                costderivative[colNo] += derivative\n",
    "        self.__constants = self.__constants-alpha*costderivative\n",
    "\n",
    "    # Training the algorithm to get coefficients having minimum cost\n",
    "    def __gradient_descent(self,x,y,alpha,iterations):\n",
    "        for count in range(iterations):\n",
    "            self.__step_gradient_descent(x,y,alpha)\n",
    "    \n",
    "    # Defining fit function which contains x as values of features, y as their corresponding output\n",
    "    # alpha is learning rate and iteration is no of rounds in the order of minimize of values of coefficient\n",
    "    # of features.\n",
    "    def fit(self,x,y,alpha,iterations):\n",
    "        self.__M = x.shape[0] # No of rows or values in the dataset\n",
    "        self.__N = x.shape[1] # No of columns or features in the dataset\n",
    "        self.__constants = np.zeros(self.__N) # Creating a np array for values of coefficients of features\n",
    "        self.__gradient_descent(x,y,alpha,iterations)\n",
    "        self.coeff = self.__constants[:-1]    # Seperating coefficients from intercept\n",
    "        self.intercept = self.__constants[-1] # Y-intercept\n",
    "    \n",
    "    # Creating Predict Function to predict the values of test inputs\n",
    "    def predict(self,test):\n",
    "        y_pred = np.zeros(self.__M)\n",
    "        for index in range(self.__M):\n",
    "            y_pred[index] = (self.__constants*test[index]).sum()\n",
    "        return y_pred\n",
    "    \n",
    "    # Score function\n",
    "    def score(self,x,y):\n",
    "        y_pred = self.predict(x)\n",
    "        u = ((y-y_pred)**2).sum()\n",
    "        v = ((y-y.mean())**2).sum()\n",
    "        return 1-u/v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf754225",
   "metadata": {},
   "source": [
    "### Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0b7b293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284, 20)\n",
      "(284,)\n",
      "Algorithm Initiated\n",
      "Self Made Algo Score: 0.7448277559664804\n",
      "Self Made Algo Predictions:\n",
      "[17.01376972 39.77341829  7.21502911 29.95814045 44.12417081 26.03397684\n",
      " 12.71600264 23.3289187  24.46286108 36.76850664 27.85221999 28.24144943\n",
      " 20.8295491  28.85656205 11.10281496 27.24211072 26.66391027 25.37115466\n",
      " 15.04746837 21.67135288  9.99034285 33.34813426 27.93158657 27.13110696\n",
      " 15.39893695 19.42604732 17.90022595 15.98461315 27.93260812 28.60329119\n",
      " 20.26537215 23.56503518 26.44995916 30.53349846 29.78303994 21.78615168\n",
      " 20.01524848 19.87793957 36.2071457  24.63252826 23.28143064 20.96787847\n",
      " 34.04727517 12.8664889  31.59650207 21.56918913 19.88423377 35.15633582\n",
      " 33.18717585 18.98006632 10.50218086 25.0285985  23.11391723 32.94185867\n",
      " 29.40089823 22.15481356 18.52958612 24.74309997 17.83087044 20.1668128\n",
      " 21.19997475 20.90957127 34.49486463 22.43892752 24.72539225 31.25205226\n",
      " -5.79998101 16.76537866 32.76426334 30.92723815 14.92696465 18.37924289\n",
      " 14.00835735 17.67075124 18.27854682  9.25735954 33.21165207  6.89943447\n",
      " 15.82641809 22.21342421 20.39432731 19.96512797 20.59623456 34.33998477\n",
      " 26.54306512 22.21335276 18.31057649 28.2592274  29.30386578 20.59102058\n",
      " 12.23666134 18.69773548 32.43713854 21.57630383  7.51817054 24.10925878\n",
      " 38.79667057 16.49300036 36.68974405  7.125604   20.6033877  13.59695529\n",
      " 15.67124166 22.46832286 27.67349796 19.90796725 29.85777397 22.05069035\n",
      " 39.68990477 29.56242413 23.61543302 27.73575063 20.09851979 32.01701928\n",
      " 16.76726389 25.46770425 25.62525377 16.49477529 19.6782156  12.98517252\n",
      " 17.40672942 31.97463509 22.54506084 24.42611812 44.58100896 39.57488774\n",
      " 24.86616232 21.52389591 24.47547004 21.83627037  9.51768394 19.12242721\n",
      "  6.00721976 29.60460065 25.80653869 29.81251273 17.38413763  8.95710898\n",
      " 19.46710518 31.30716934 34.35585309 30.27552239 23.18889802 21.68558944\n",
      "  4.12878159 17.73167958 26.00910754 19.3865373  12.29744852 23.08854927\n",
      " 23.21932898  9.46281767 20.8981717  22.6745106  23.96077164 38.13548719\n",
      " 17.96604636 21.32643723 25.03931439 28.25396821 14.85536116 19.10114832\n",
      "  4.36546531 21.40644358 27.079036   18.50975108 20.03923147 16.77527637\n",
      " 41.26595678 17.36020739 21.19692225 34.11505312 12.60814704 14.70885656\n",
      " 31.6208395  23.99478814 19.73108471 24.32603418 38.67487773 22.90494188\n",
      " 25.61591025 25.65240553 17.72011341  7.69689275 31.16631502 18.5841653\n",
      " 14.74531825  8.87806271 26.18898783 21.60876869 14.52242778 19.46180989\n",
      " 35.55631352 33.1259581  15.38123627 43.32098531 24.11213781 30.8528015\n",
      " 12.51947803 19.13158815 25.11103678 19.47834011 19.32327093 24.97529427\n",
      " 25.49055778  8.40976862 30.77425886 20.2131967  27.90945021 20.28144279\n",
      " 18.03288029 23.28239407 12.79236261 37.22099567  3.8998121  17.0271465\n",
      " 36.93339463 31.91198752 25.91175437 34.57056199 13.05169366 13.25684358\n",
      " 21.71124968 26.37566167 24.19039868 19.94456808 28.60129673 17.115813\n",
      " 36.43473851 27.16460149 20.02399301 15.65457657 16.87670574 19.71756735\n",
      " 20.60375963 13.51010936 20.95912447 12.31102639 16.4596127   9.22750428\n",
      " 38.74419823 19.62549163 11.22846598 13.52823767 23.84999299 26.83360004\n",
      " 37.58571949 26.25318486 15.81362598 21.34088179 21.66945207 23.87545415\n",
      " 16.04476606 24.97756755 34.60694657 35.60147971 33.57081028 23.95905042\n",
      " 21.52628008 17.07487068 12.72764682 22.72228133 33.05709681 19.13037212\n",
      " 13.55747999 13.88975813 25.1423914  26.83531217 34.14835953 27.8720569\n",
      " 25.84363734 17.25477058 33.47937593 13.75829398 30.5152609  16.08285609\n",
      " 15.92926676 20.87634306 13.49287843 13.26235655 23.860433   10.55977113\n",
      " 35.06189876 15.21653979]\n"
     ]
    }
   ],
   "source": [
    "# Appending the column containing 1s to end of the x\n",
    "ones = np.ones(x.shape[0]).reshape(x.shape[0],1)\n",
    "x = np.append(x,ones,axis = 1)\n",
    "# Splitting the x,y into x_train,x_test,y_train,y_test for testing the algorithm\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "algo = LinearRegression()\n",
    "algo.fit(x_train,y_train,0.02,500)\n",
    "print('Self Made Algo Score:', algo.score(x_train,y_train))\n",
    "print('Self Made Algo Predictions:')\n",
    "print(algo.predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925b183",
   "metadata": {},
   "source": [
    "### Comparing with Linear Regression in SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cfcb361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inbuilt algo score: 0.7450606468058916\n",
      "Inbuilt algo predictions:\n",
      "[17.0996055  39.77186499  7.1792253  30.1181784  44.10073796 26.08142335\n",
      " 12.78157538 23.36219083 24.4804286  36.77265792 27.73795027 28.18965494\n",
      " 20.84401057 28.860753   11.02562881 27.26140374 26.19980977 25.34798721\n",
      " 14.98818858 21.52165885 10.02596738 33.21378262 27.98432967 27.15829294\n",
      " 15.03695254 19.27843799 17.92090662 15.90825807 28.25905734 28.6744422\n",
      " 20.36439081 23.56716958 26.4037441  30.57211125 29.8656989  21.61619623\n",
      " 19.83666148 19.96024649 36.13776961 24.71995042 23.31931143 21.09045665\n",
      " 34.12241696 12.84565843 31.85959556 21.66066654 19.79098133 35.1939373\n",
      " 33.3703234  18.99754128 10.53137669 25.1242361  23.24916593 32.80132352\n",
      " 29.45604496 22.22713581 18.57233934 24.72479949 18.00182356 20.07040416\n",
      " 21.37351491 20.81265895 34.73805114 22.5160857  24.79859629 31.51890219\n",
      " -5.80547202 16.8377151  33.03161214 30.71002951 14.83164488 18.25359269\n",
      " 13.94595193 17.57454095 18.30808755  9.28593099 33.20879209  6.95022769\n",
      " 15.74583458 22.19911499 20.97701145 19.96736521 20.60244212 34.12568627\n",
      " 26.72817962 22.28588732 18.16175385 28.35089486 29.32164469 20.59406378\n",
      " 12.30645663 18.69220967 32.65259562 21.69167071  7.45875952 24.18176999\n",
      " 38.83151771 16.5276664  36.70009329  6.99365399 20.41660921 13.67160998\n",
      " 15.6479743  22.11576392 27.44755911 19.89210797 29.94917013 22.14471751\n",
      " 39.73550676 29.52444629 23.63150531 28.06237426 20.10657129 31.96128783\n",
      " 16.83427987 25.81489648 25.41853469 16.4150706  19.73946173 12.99194321\n",
      " 17.40154496 31.94870508 22.52606482 24.33380443 44.72520168 39.60043887\n",
      " 24.77755748 21.61592928 24.0962219  21.64021956  9.61052224 19.20361566\n",
      "  6.0362712  29.52147421 25.75955643 30.1012711  17.54421166  8.88373533\n",
      " 19.48251887 31.1144529  34.53521058 30.06671252 23.21140903 21.62815623\n",
      "  4.23776632 17.81535535 25.9690498  19.29467608 12.37882086 23.11368011\n",
      " 23.32479932  8.91378392 20.71721826 22.70721904 23.94071132 38.15170551\n",
      " 17.84070743 21.27836166 25.14190079 28.46051363 14.82330637 19.11527673\n",
      "  3.82395607 21.28358798 27.00061441 18.57606549 20.1230774  16.56764545\n",
      " 41.28469798 17.39531368 21.00376959 34.1488024  12.63218731 14.62234818\n",
      " 31.61599656 24.10562503 19.8303662  24.45951293 38.67911191 22.95022682\n",
      " 25.61651476 25.64924442 17.76549808  7.64126252 31.19215199 18.60529428\n",
      " 14.70916204  8.9528887  26.20298401 21.49752277 14.4159686  19.63610959\n",
      " 35.60140407 33.18848132 15.48744821 43.25069255 24.21659871 30.8129454\n",
      " 12.54075897 19.12623203 25.07793572 19.51244886 19.36176239 25.28812393\n",
      " 25.44882035  8.38692645 30.8077194  20.29787916 28.07614646 20.31119702\n",
      " 18.03388184 23.36923368 12.82580187 37.24636888  3.88005261 16.86159592\n",
      " 36.72714358 31.70350774 25.91598452 34.5885161  13.05054401 13.2816536\n",
      " 21.80515922 26.36079941 24.01130958 20.51793447 28.70568796 16.99729728\n",
      " 36.51424284 27.17326458 19.93283829 15.67134914 16.72147269 19.71006348\n",
      " 20.61402634 14.09363614 20.8014456  12.44548815 16.52682145  9.41636625\n",
      " 38.73702906 19.61400637 11.23072903 13.49126109 23.88684275 26.88476991\n",
      " 37.38785342 26.24394262 15.72965312 21.39328398 21.64693447 23.95605272\n",
      " 16.04323983 24.88769509 34.53778928 35.69108666 33.39502608 23.49630277\n",
      " 21.42491812 17.14646859 12.75357514 22.36618669 33.13506167 19.14465902\n",
      " 13.00890825 13.92276191 25.07736648 26.62163989 34.1517377  27.75664382\n",
      " 25.64954663 17.35986361 33.48116524 13.87838204 30.66590846 16.0046498\n",
      " 15.99590048 20.88876332 13.42524721 13.43277192 23.81089301 10.66364215\n",
      " 34.89198656 15.06671611]\n"
     ]
    }
   ],
   "source": [
    "inbuilt_algo = linear_model.LinearRegression()\n",
    "inbuilt_algo.fit(x_train,y_train)\n",
    "print('Inbuilt algo score:', inbuilt_algo.score(x_train,y_train))\n",
    "print('Inbuilt algo predictions:')\n",
    "print(inbuilt_algo.predict(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

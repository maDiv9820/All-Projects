{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ef7b93d3",
   "metadata": {},
   "source": [
    "Question 1)\n",
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "\n",
    "1. Code Gradient Descent for N features and come with predictions.\n",
    "2. Try and test with various combinations of learning rates and number of iterations.\n",
    "3. Try using Feature Scaling, and see if it helps you in getting better results. \n",
    "\n",
    "Read Instructions carefully -\n",
    "\n",
    "1. Use Gradient Descent as a training algorithm and submit results predicted.\n",
    "2. Files are in csv format, you can use genfromtxt function in numpy to load data from csv file. Similarly you can use savetxt function to save data into a file.\n",
    "3. Submit a csv file with only predictions for X test data. File name should not have spaces. File should not have any headers and should only have one column i.e. predictions. Also predictions shouldn't be in exponential form. \n",
    "4. Your score is based on coefficient of determination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd2acf",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2e460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # For creating nd arrays\n",
    "from sklearn.model_selection import train_test_split # For random spliting of dataset\n",
    "from sklearn import linear_model # For using inbuilt LinearRegression in the Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf83ba4",
   "metadata": {},
   "source": [
    "### Fetching data from Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c69add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40784991, -0.48772236, -1.2660231 , ...,  0.41057102,\n",
       "        -1.09799011, 37.9       ],\n",
       "       [-0.40737368, -0.48772236,  0.24705682, ...,  0.29116915,\n",
       "        -0.52047412, 21.4       ],\n",
       "       [ 0.1251786 , -0.48772236,  1.01599907, ..., -3.79579542,\n",
       "         0.89107588, 12.7       ],\n",
       "       ...,\n",
       "       [-0.40831101, -0.48772236,  0.24705682, ...,  0.33206621,\n",
       "        -0.33404299, 20.8       ],\n",
       "       [-0.41061997, -0.48772236, -1.15221381, ...,  0.203235  ,\n",
       "        -0.74475218, 22.6       ],\n",
       "       [ 0.34290895, -0.48772236,  1.01599907, ...,  0.38787479,\n",
       "        -1.35871335, 50.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Set for training the algorithm\n",
    "train_set = np.loadtxt('boston_train.csv', delimiter = ',')\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123ac5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.91816626, -0.48772236,  1.01599907, ...,  0.80657583,\n",
       "        -1.59755122,  1.04106182],\n",
       "       [-0.40339151, -0.48772236,  0.40609801, ..., -1.13534664,\n",
       "         0.44105193, -0.89473812],\n",
       "       [-0.4131781 , -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.50084979],\n",
       "       ...,\n",
       "       [-0.41001449,  2.08745172, -1.37837329, ..., -0.0719129 ,\n",
       "         0.39094481, -0.68167397],\n",
       "       [-0.40317611, -0.48772236, -0.37597609, ...,  1.13022958,\n",
       "         0.34007019,  0.20142086],\n",
       "       [-0.13356344, -0.48772236,  1.2319449 , ..., -1.73641788,\n",
       "        -2.93893082,  0.48877712]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Set for testing the algorithm\n",
    "test_set = np.loadtxt('boston_test.csv', delimiter = ',')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cee58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_set[:,:-1] # Fetching all the parameters from training_set\n",
    "y = train_set[:,-1]  # Fetching all the output from training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca0b64",
   "metadata": {},
   "source": [
    "### Creating Class for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d14acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating class of Linear Regression for n features using Gradient Descent\n",
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        print('Algorithm Initiated')\n",
    "    \n",
    "    # Defining the cost function\n",
    "    def __cost(self,x,y):\n",
    "        try:\n",
    "            cost = np.zeros(self.__M)\n",
    "            for rowNo in range(self.__M):\n",
    "                cost[rowNo] = (y[rowNo]-((self.__constants*x[rowNo]).sum()))**2\n",
    "            return cost.mean()\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "    \n",
    "    # Defining function for gradient descent for minimizing the cost\n",
    "    def __step_gradient_descent(self,x,y,alpha):\n",
    "        try:\n",
    "            costderivative = np.zeros(self.__N)\n",
    "            for colNo in range(self.__N):\n",
    "                for rowNo in range(self.__M):\n",
    "                    # Adding to derivative of cost with respect to each coefficient\n",
    "                    derivative = (-2/self.__M)*(y[rowNo]-((self.__constants*x[rowNo]).sum()))*x[rowNo,colNo]\n",
    "                    costderivative[colNo] += derivative\n",
    "            self.__constants = self.__constants-alpha*costderivative\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "\n",
    "    # Training the algorithm to get coefficients having minimum cost\n",
    "    def __gradient_descent(self,x,y,alpha,iterations):\n",
    "        try:\n",
    "            for count in range(iterations):\n",
    "                self.__step_gradient_descent(x,y,alpha)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "    \n",
    "    # Defining fit function which contains x as values of features, y as their corresponding output\n",
    "    # alpha is learning rate and iteration is no of rounds in the order of minimize of values of coefficient\n",
    "    # of features.\n",
    "    def fit(self,x,y,alpha,iterations):\n",
    "        try:\n",
    "            self.__M = x.shape[0] # No of rows or values in the dataset\n",
    "            self.__N = x.shape[1] # No of columns or features in the dataset\n",
    "            self.__constants = np.zeros(self.__N) # Creating a np array for values of coefficients of features\n",
    "            self.__gradient_descent(x,y,alpha,iterations)\n",
    "            self.coeff = self.__constants[:-1]    # Seperating coefficients from intercept\n",
    "            self.intercept = self.__constants[-1] # Y-intercept\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "    \n",
    "    # Creating Predict Function to predict the values of test inputs\n",
    "    def predict(self,test):\n",
    "        try:\n",
    "            y_pred = np.zeros(test.shape[0])\n",
    "            for index in range(test.shape[0]):\n",
    "                y_pred[index] = (self.__constants*test[index]).sum()\n",
    "            return y_pred\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "    \n",
    "    # Score function\n",
    "    def score(self,x,y):\n",
    "        try:\n",
    "            y_pred = self.predict(x)\n",
    "            u = ((y-y_pred)**2).sum()\n",
    "            v = ((y-y.mean())**2).sum()\n",
    "            return 1-u/v\n",
    "        except Exception as e:\n",
    "            print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d57827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14)\n",
      "(127, 14)\n"
     ]
    }
   ],
   "source": [
    "# Appending the column containing 1s to end of the x\n",
    "ones1 = np.ones(x.shape[0]).reshape(x.shape[0],1)\n",
    "x = np.append(x,ones1,axis = 1)\n",
    "ones2 = np.ones(test_set.shape[0]).reshape(test_set.shape[0],1)\n",
    "test_set = np.append(test_set,ones2,axis=1)\n",
    "print(x.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf754225",
   "metadata": {},
   "source": [
    "### Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b7b293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Initiated\n",
      "Self Made Algo Score: 0.7448201427106371\n",
      "Self Made Algo Predictions:\n",
      "[17.01567138 39.77760426  7.21601189 29.94682599 44.1253906  26.03476934\n",
      " 12.71312931 23.33630762 24.46265181 36.76500889 27.85060724 28.23212382\n",
      " 20.83306722 28.85416055 11.10487519 27.2359065  26.66683838 25.37606074\n",
      " 15.04731412 21.66845249  9.98829608 33.33763536 27.93216052 27.12893247\n",
      " 15.39832196 19.42441584 17.90067411 15.98611379 27.92306391 28.60100743\n",
      " 20.26344586 23.56992979 26.45386343 30.53002221 29.78305366 21.78152239\n",
      " 20.01178474 19.87650208 36.2053684  24.62992161 23.27785042 20.96930602\n",
      " 34.04140061 12.86943163 31.59175287 21.56297334 19.88730759 35.15340512\n",
      " 33.17849616 18.98290989 10.5079858  25.02322229 23.10973811 32.96023701\n",
      " 29.39168725 22.15130216 18.52931904 24.74327198 17.83231634 20.17095569\n",
      " 21.19306686 20.91387007 34.4842739  22.43872465 24.7241418  31.23949566\n",
      " -5.80267153 16.76222424 32.75222428 30.92552987 14.93033682 18.37474175\n",
      " 14.00803611 17.67470535 18.27878229  9.2548917  33.20959039  6.89662696\n",
      " 15.83325657 22.22063653 20.38974396 19.96942988 20.60399549 34.34172147\n",
      " 26.54060051 22.21180537 18.30386725 28.26173223 29.30704779 20.59526758\n",
      " 12.23347398 18.70167549 32.4292496  21.57321572  7.52457766 24.11092853\n",
      " 38.79708511 16.497882   36.68845925  7.13027112 20.59920256 13.5936185\n",
      " 15.67494702 22.47953857 27.67310298 19.91121238 29.85309558 22.04777492\n",
      " 39.68605448 29.55746129 23.6121612  27.72771132 20.10022498 32.01274159\n",
      " 16.76342552 25.45922781 25.62402837 16.49629029 19.67414758 12.98614475\n",
      " 17.40793492 31.97889667 22.5426781  24.43288218 44.57315751 39.574905\n",
      " 24.8688544  21.52367652 24.48839716 21.84651573  9.51193715 19.11974609\n",
      "  6.00338877 29.6115085  25.80140453 29.79835611 17.38593358  8.9645255\n",
      " 19.4734286  31.3073099  34.35240027 30.27324658 23.19177019 21.68762612\n",
      "  4.12145198 17.73429995 26.01255258 19.38946773 12.2915674  23.08473666\n",
      " 23.21739807  9.46691104 20.89387102 22.68199671 23.96630319 38.13789735\n",
      " 17.96682998 21.32786167 25.03232512 28.25404833 14.85794583 19.10368839\n",
      "  4.36897562 21.41226663 27.09179569 18.50628524 20.03931815 16.78603189\n",
      " 41.26435997 17.3605549  21.19287701 34.1124598  12.60738724 14.71725068\n",
      " 31.62091754 23.99168914 19.72388478 24.33014753 38.67796384 22.90357985\n",
      " 25.61620487 25.65292106 17.71003166  7.7028097  31.15995426 18.58441178\n",
      " 14.74754532  8.87438237 26.18898203 21.61038078 14.52942459 19.46571837\n",
      " 35.55212754 33.11948479 15.37680875 43.32621663 24.10645619 30.85269212\n",
      " 12.51969765 19.14053455 25.11267376 19.4773536  19.32314197 24.98663343\n",
      " 25.49835161  8.41170848 30.77060684 20.2111617  27.9029202  20.28128183\n",
      " 18.03592153 23.29093932 12.7904379  37.21888538  3.90130708 17.02442826\n",
      " 36.93288253 31.912881   25.91020667 34.56616428 13.05210674 13.26246733\n",
      " 21.70793642 26.37791934 24.20186115 19.94032037 28.59696619 17.12361854\n",
      " 36.43130155 27.15934401 20.02715745 15.64837617 16.8705678  19.72213622\n",
      " 20.60304279 13.50599392 20.9555086  12.30230291 16.45422672  9.21159871\n",
      " 38.74911093 19.63000267 11.22979743 13.53270846 23.84508418 26.83364284\n",
      " 37.58685559 26.24404361 15.817087   21.34538294 21.6706197  23.86492168\n",
      " 16.04683554 24.97710634 34.60542693 35.59602863 33.56923532 23.96427148\n",
      " 21.53645598 17.07276712 12.72804176 22.73709909 33.05358214 19.13367045\n",
      " 13.56157625 13.88751489 25.15754057 26.83433312 34.14598828 27.86701309\n",
      " 25.83957477 17.25352595 33.47693871 13.74815168 30.51511542 16.08582066\n",
      " 15.92649531 20.87822839 13.49341158 13.24931421 23.86268611 10.55432099\n",
      " 35.0561846  15.22523957]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the x,y into x_train,x_test,y_train,y_test for testing the algorithm\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 1)\n",
    "algo = LinearRegression()\n",
    "algo.fit(x_train,y_train,0.02,500)\n",
    "print('Self Made Algo Score:', algo.score(x_train,y_train))\n",
    "print('Self Made Algo Predictions:')\n",
    "print(algo.predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d9b0e",
   "metadata": {},
   "source": [
    "### Comparing with Linear Regression in SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b25a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inbuilt algo score: 0.7450606468058916\n",
      "Inbuilt algo predictions:\n",
      "[17.0996055  39.77186499  7.1792253  30.1181784  44.10073796 26.08142335\n",
      " 12.78157538 23.36219083 24.4804286  36.77265792 27.73795027 28.18965494\n",
      " 20.84401057 28.860753   11.02562881 27.26140374 26.19980977 25.34798721\n",
      " 14.98818858 21.52165885 10.02596738 33.21378262 27.98432967 27.15829294\n",
      " 15.03695254 19.27843799 17.92090662 15.90825807 28.25905734 28.6744422\n",
      " 20.36439081 23.56716958 26.4037441  30.57211125 29.8656989  21.61619623\n",
      " 19.83666148 19.96024649 36.13776961 24.71995042 23.31931143 21.09045665\n",
      " 34.12241696 12.84565843 31.85959556 21.66066654 19.79098133 35.1939373\n",
      " 33.3703234  18.99754128 10.53137669 25.1242361  23.24916593 32.80132352\n",
      " 29.45604496 22.22713581 18.57233934 24.72479949 18.00182356 20.07040416\n",
      " 21.37351491 20.81265895 34.73805114 22.5160857  24.79859629 31.51890219\n",
      " -5.80547202 16.8377151  33.03161214 30.71002951 14.83164488 18.25359269\n",
      " 13.94595193 17.57454095 18.30808755  9.28593099 33.20879209  6.95022769\n",
      " 15.74583458 22.19911499 20.97701145 19.96736521 20.60244212 34.12568627\n",
      " 26.72817962 22.28588732 18.16175385 28.35089486 29.32164469 20.59406378\n",
      " 12.30645663 18.69220967 32.65259562 21.69167071  7.45875952 24.18176999\n",
      " 38.83151771 16.5276664  36.70009329  6.99365399 20.41660921 13.67160998\n",
      " 15.6479743  22.11576392 27.44755911 19.89210797 29.94917013 22.14471751\n",
      " 39.73550676 29.52444629 23.63150531 28.06237426 20.10657129 31.96128783\n",
      " 16.83427987 25.81489648 25.41853469 16.4150706  19.73946173 12.99194321\n",
      " 17.40154496 31.94870508 22.52606482 24.33380443 44.72520168 39.60043887\n",
      " 24.77755748 21.61592928 24.0962219  21.64021956  9.61052224 19.20361566\n",
      "  6.0362712  29.52147421 25.75955643 30.1012711  17.54421166  8.88373533\n",
      " 19.48251887 31.1144529  34.53521058 30.06671252 23.21140903 21.62815623\n",
      "  4.23776632 17.81535535 25.9690498  19.29467608 12.37882086 23.11368011\n",
      " 23.32479932  8.91378392 20.71721826 22.70721904 23.94071132 38.15170551\n",
      " 17.84070743 21.27836166 25.14190079 28.46051363 14.82330637 19.11527673\n",
      "  3.82395607 21.28358798 27.00061441 18.57606549 20.1230774  16.56764545\n",
      " 41.28469798 17.39531368 21.00376959 34.1488024  12.63218731 14.62234818\n",
      " 31.61599656 24.10562503 19.8303662  24.45951293 38.67911191 22.95022682\n",
      " 25.61651476 25.64924442 17.76549808  7.64126252 31.19215199 18.60529428\n",
      " 14.70916204  8.9528887  26.20298401 21.49752277 14.4159686  19.63610959\n",
      " 35.60140407 33.18848132 15.48744821 43.25069255 24.21659871 30.8129454\n",
      " 12.54075897 19.12623203 25.07793572 19.51244886 19.36176239 25.28812393\n",
      " 25.44882035  8.38692645 30.8077194  20.29787916 28.07614646 20.31119702\n",
      " 18.03388184 23.36923368 12.82580187 37.24636888  3.88005261 16.86159592\n",
      " 36.72714358 31.70350774 25.91598452 34.5885161  13.05054401 13.2816536\n",
      " 21.80515922 26.36079941 24.01130958 20.51793447 28.70568796 16.99729728\n",
      " 36.51424284 27.17326458 19.93283829 15.67134914 16.72147269 19.71006348\n",
      " 20.61402634 14.09363614 20.8014456  12.44548815 16.52682145  9.41636625\n",
      " 38.73702906 19.61400637 11.23072903 13.49126109 23.88684275 26.88476991\n",
      " 37.38785342 26.24394262 15.72965312 21.39328398 21.64693447 23.95605272\n",
      " 16.04323983 24.88769509 34.53778928 35.69108666 33.39502608 23.49630277\n",
      " 21.42491812 17.14646859 12.75357514 22.36618669 33.13506167 19.14465902\n",
      " 13.00890825 13.92276191 25.07736648 26.62163989 34.1517377  27.75664382\n",
      " 25.64954663 17.35986361 33.48116524 13.87838204 30.66590846 16.0046498\n",
      " 15.99590048 20.88876332 13.42524721 13.43277192 23.81089301 10.66364215\n",
      " 34.89198656 15.06671611]\n"
     ]
    }
   ],
   "source": [
    "inbuilt_algo = linear_model.LinearRegression()\n",
    "inbuilt_algo.fit(x_train,y_train)\n",
    "print('Inbuilt algo score:', inbuilt_algo.score(x_train,y_train))\n",
    "print('Inbuilt algo predictions:')\n",
    "print(inbuilt_algo.predict(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77078f",
   "metadata": {},
   "source": [
    "### Predicting Output for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be3a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm Initiated\n"
     ]
    }
   ],
   "source": [
    "# Creating a new object of LinearRegression algorithm that we created\n",
    "algo1 = LinearRegression()\n",
    "algo1.fit(x,y,0.02,500) # Training as per given inputs\n",
    "y_pred = algo1.predict(test_set) # Predicting the output for test_set\n",
    "y_pred = np.round(y_pred,5) # Rounding off the values to the 5 decimal places\n",
    "np.savetxt('ans.csv',y_pred,delimiter = ',') # Saving the predictions into the csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
